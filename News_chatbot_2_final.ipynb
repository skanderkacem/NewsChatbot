{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZX_WpmTSfMY"
      },
      "source": [
        "# üß† News Chatbot ‚Äî Advanced Deep Learning Project\n",
        "\n",
        "This project presents an **AI-powered news chatbot** developed as part of the *Advanced Deep Learning* course.  \n",
        "The chatbot leverages **LangChain** and **Retrieval-Augmented Generation (RAG)** to answer user queries based on **real-world news articles** fetched via RSS feeds and APIs.\n",
        "\n",
        "Our goal is to build an intelligent conversational agent capable of:\n",
        "- Understanding user questions in natural language\n",
        "- Retrieving the most relevant information from recent news\n",
        "- Generating coherent, factual, and source-based answers\n",
        "\n",
        "The project demonstrates how **LLMs (Qwen 1.5B)** can be combined with **embeddings (MiniLM-L6-v2)** and **vector databases (FAISS)** to create reliable, domain-specific agents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4TYNS8QSfMa"
      },
      "source": [
        "### üßë‚Äçüíª Authors\n",
        "\n",
        "- üë®‚Äçüíª Guenichi Ibrahim\n",
        "- üë®‚Äçüíª Ben Mohamed Malak\n",
        "- üë®‚Äçüíª Kacem Skander\n",
        "- üë®‚Äçüíª Abdellaoui Malek\n",
        "- üë®‚Äçüíª Bettaieb Ahmed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3J9spRSSfMa"
      },
      "source": [
        "### üìò Notebook Structure\n",
        "\n",
        "1. **Framework Used ‚Äî LangChain**\n",
        "2. **Data Collection & Preprocessing**\n",
        "3. **Embedding Creation**\n",
        "4. **Vector Store (FAISS) Setup**\n",
        "5. **LLM & Prompt Template**\n",
        "6. **RAG Pipeline Implementation**\n",
        "7. **Fine-tuning and Optimization**\n",
        "8. **Model Architecture Summary**\n",
        "9. **Demo of the Chatbot**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JinQCFCdSfMa"
      },
      "source": [
        "## üîß Environment Setup and Imports\n",
        "\n",
        "This section imports all necessary frameworks and libraries used throughout the project.  \n",
        "The main framework powering the chatbot is **LangChain**, which provides tools for prompt templates, chains, retrievers, and agents.  \n",
        "\n",
        "Additional packages such as **HuggingFace Transformers** are used for the language model pipeline, while **FAISS** supports fast similarity searches for document retrieval.  \n",
        "Together, these components form the foundation of our Retrieval-Augmented Generation (RAG) system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRoTwtPC1IOo"
      },
      "source": [
        "### Install required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoMvsP9c07UU",
        "outputId": "658fff49-3eb8-473b-d833-cb7fadee59bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/81.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Libraries installed successfully: feedparser, requests\n"
          ]
        }
      ],
      "source": [
        "# Install required Python libraries for data ingestion\n",
        "# Purpose: Ensure dependencies are available in Google Colab\n",
        "!pip install feedparser requests --quiet\n",
        "print(\"Libraries installed successfully: feedparser, requests\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVp8BTgjSfMc"
      },
      "source": [
        "#### Additional preprocessing libraries\n",
        "Installs HTML parsing, tokenization and data utilities used during preprocessing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Y6HTIVFPPPWO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30016f5a-b04e-4519-f335-bf3fa95ab52b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 nltk rouge-score datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "R1Eco8sPSWLT"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.config/Google"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyyN9vzW1Byw"
      },
      "source": [
        "#### Imports and Drive mount\n",
        "Imports core libraries and mounts Google Drive for persistent storage when running in Colab. Replace or guard the drive mount when running locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOk2RdFa1OeZ",
        "outputId": "d2d78c0d-fd52-4603-b009-639f0418d49c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Environment setup complete. Directory: /content/drive/MyDrive/news_data\n"
          ]
        }
      ],
      "source": [
        "# Import libraries and mount Google Drive for persistent storage\n",
        "import feedparser\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from google.colab import drive\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# Mount Google Drive and create base directory\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "BASE_DIR = '/content/drive/MyDrive/news_data'\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "logging.info(f\"Google Drive mounted. Base directory created at: {BASE_DIR}\")\n",
        "print(f\"Environment setup complete. Directory: {BASE_DIR}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETF3ydZY1Z5M"
      },
      "source": [
        "### üåê Defining Categories and Data Sources (RSS / API)\n",
        "\n",
        "This section centralizes the configuration of all **news categories** and their corresponding **RSS feed URLs** or **API endpoints**.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRZGOhvR1crC",
        "outputId": "7ade2e96-6c98-40dc-c59d-0ac4500f45b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Defined categories: world, politics, business, technology, science, entertainment, sports, health\n",
            "Configured 24 RSS feeds and 2 APIs\n"
          ]
        }
      ],
      "source": [
        "# Define news categories and data sources\n",
        "CATEGORIES = ['world', 'politics', 'business', 'technology', 'science', 'entertainment', 'sports', 'health']\n",
        "\n",
        "RSS_FEEDS = {\n",
        "    'world': ['https://feeds.bbci.co.uk/news/world/rss.xml', 'https://www.reuters.com/arc/outboundfeeds/world/?outputType=xml', 'http://rss.cnn.com/rss/cnn_world.rss'],\n",
        "    'politics': ['https://www.theguardian.com/politics/rss', 'https://www.politico.com/rss/politicopulse.xml', 'https://www.aljazeera.com/xml/rss/all.xml'],\n",
        "    'business': ['https://feeds.bloomberg.com/news/business.rss', 'https://www.ft.com/rss/home', 'https://www.cnbc.com/id/100727362/device/rss/rss.html'],\n",
        "    'technology': ['https://techcrunch.com/feed/', 'https://www.wired.com/feed/rss', 'https://www.theverge.com/rss/index.xml'],\n",
        "    'science': ['https://www.nature.com/nature.rss', 'https://www.science.org/rss/news_current.xml', 'https://www.newscientist.com/feed/home/?cmpid=RSS%7CNSNS%7Cnews'],\n",
        "    'entertainment': ['https://variety.com/feed/', 'https://www.hollywoodreporter.com/feed/', 'https://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml'],\n",
        "    'sports': ['https://www.espn.com/espn/rss/news', 'https://feeds.bbci.co.uk/sport/rss.xml', 'https://www.skysports.com/rss/0,20514,11095,00.xml'],\n",
        "    'health': ['https://www.who.int/feeds/entity/mediacentre/news/en/rss.xml', 'https://www.reuters.com/arc/outboundfeeds/health/?outputType=xml', 'https://newsnetwork.mayoclinic.org/feed/']\n",
        "}\n",
        "\n",
        "NEWSAPI_KEY = 'd1fc7716158e4ecf86b9bb812a3b9fcf'\n",
        "NEWSDATA_KEY = 'pub_f344b9f77d3645aabb54c0e52cb051fd'\n",
        "NEWSAPI_ENDPOINT = 'https://newsapi.org/v2/top-headlines'\n",
        "NEWSDATA_ENDPOINT = 'https://newsdata.io/api/1/news'\n",
        "\n",
        "print(f\"Defined categories: {', '.join(CATEGORIES)}\")\n",
        "print(f\"Configured {sum(len(feeds) for feeds in RSS_FEEDS.values())} RSS feeds and 2 APIs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tBC1zIjQ2Ybr"
      },
      "source": [
        "### üì° RSS Fetch Function\n",
        "\n",
        "This function parses a single **RSS feed** and converts each entry into a **normalized article dictionary** containing:\n",
        "- `title`  \n",
        "- `description`  \n",
        "- `content`  \n",
        "- `link`  \n",
        "- `pub_date`  \n",
        "- `source`  \n",
        "\n",
        "By unifying the format of all feeds, this step enables seamless integration of RSS content with API-based articles in later stages of the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RcZ_kQD32bpy",
        "outputId": "52563ca9-1afc-44b1-a046-35d51274625a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: Fetched 36 articles from https://feeds.bbci.co.uk/news/world/rss.xml\n"
          ]
        }
      ],
      "source": [
        "# Fetch articles from a single RSS feed\n",
        "def fetch_rss(feed_url):\n",
        "    try:\n",
        "        feed = feedparser.parse(feed_url)\n",
        "        articles = []\n",
        "        for entry in feed.entries:\n",
        "            article = {\n",
        "                'title': entry.get('title', 'No Title'),\n",
        "                'description': entry.get('description', 'No Description'),\n",
        "                'content': entry.get('content', [{}])[0].get('value', 'No Content'),\n",
        "                'link': entry.get('link', 'No Link'),\n",
        "                'pub_date': entry.get('published', datetime.now().isoformat()),\n",
        "                'source': feed.feed.get('title', feed_url.split('/')[2])\n",
        "            }\n",
        "            articles.append(article)\n",
        "        logging.info(f\"Fetched {len(articles)} articles from RSS: {feed_url}\")\n",
        "        print(f\"Success: Fetched {len(articles)} articles from {feed_url}\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logging.error(f\"RSS fetch error for {feed_url}: {str(e)}\")\n",
        "        print(f\"Error fetching {feed_url}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Test RSS fetch\n",
        "sample_articles = fetch_rss('https://feeds.bbci.co.uk/news/world/rss.xml')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPtLlfIF2gWC"
      },
      "source": [
        "### üåç NewsAPI Fetch Function\n",
        "\n",
        "This function connects to the **NewsAPI** service to collect news articles based on a specified category.  \n",
        "It standardizes the returned data into a consistent structure used across all sources.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFN8-v_G2jbM",
        "outputId": "e42f2b09-95ca-46ab-a894-be8b05e6565a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: Fetched 20 articles from NewsAPI for technology\n"
          ]
        }
      ],
      "source": [
        "# Fetch articles from NewsAPI.org for a category\n",
        "def fetch_newsapi(category):\n",
        "    params = {'category': category, 'apiKey': NEWSAPI_KEY, 'pageSize': 20, 'language': 'en'}\n",
        "    try:\n",
        "        response = requests.get(NEWSAPI_ENDPOINT, params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "        if data.get('status') != 'ok':\n",
        "            raise ValueError(f\"API error: {data.get('message')}\")\n",
        "        articles = []\n",
        "        for item in data.get('articles', []):\n",
        "            article = {\n",
        "                'title': item.get('title', 'No Title'),\n",
        "                'description': item.get('description', 'No Description'),\n",
        "                'content': item.get('content', 'No Content'),\n",
        "                'link': item.get('url', 'No Link'),\n",
        "                'pub_date': item.get('publishedAt', datetime.now().isoformat()),\n",
        "                'source': item['source'].get('name', 'NewsAPI')\n",
        "            }\n",
        "            articles.append(article)\n",
        "        logging.info(f\"Fetched {len(articles)} articles from NewsAPI for {category}\")\n",
        "        print(f\"Success: Fetched {len(articles)} articles from NewsAPI for {category}\")\n",
        "        return articles\n",
        "    except Exception as e:\n",
        "        logging.error(f\"NewsAPI fetch error for {category}: {str(e)}\")\n",
        "        print(f\"Error fetching NewsAPI for {category}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Test NewsAPI fetch\n",
        "sample_articles = fetch_newsapi('technology')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRkcaevF2sWP"
      },
      "source": [
        "### üì∞ NewsData.io Fetch Function\n",
        "\n",
        "This function queries the **NewsData.io API** using predefined category mappings to retrieve the latest news articles.  \n",
        "It includes built-in error handling for common API response issues such as **422 (Invalid Query)** and **429 (Rate Limit Exceeded)**, ensuring reliable and continuous data collection even under network or quota constraints.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaR7LpnG2ood",
        "outputId": "fdf92a5b-7642-480d-d282-2b8f147e00c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: Fetched 10 articles from NewsData.io for technology (mapped to 'technology')\n",
            "Sample article count: 10\n",
            "First article title: Why laser toning works best for Indian skin\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from datetime import datetime\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def fetch_newsdata(category, country=None):\n",
        "    \"\"\"\n",
        "    Fetch articles from NewsData.io for a given category.\n",
        "\n",
        "    Args:\n",
        "        category (str): News category (e.g., 'technology', 'business').\n",
        "        country (str, optional): Country code (e.g., 'us'; None for global).\n",
        "\n",
        "    Returns:\n",
        "        list: List of article dictionaries or empty list on error.\n",
        "\n",
        "    Note:\n",
        "        Uses known supported categories: general, business, entertainment, health,\n",
        "        science, sports, technology, politics. Maps 'world' to 'general'.\n",
        "        Get a personal API key from https://newsdata.io/ for higher limits (500 calls/day).\n",
        "    \"\"\"\n",
        "    # Known supported categories (based on NewsData.io docs)\n",
        "    SUPPORTED_CATEGORIES = [\n",
        "        'general', 'business', 'entertainment', 'health',\n",
        "        'science', 'sports', 'technology', 'politics'\n",
        "    ]\n",
        "\n",
        "    # Map input categories to API-supported values\n",
        "    category_map = {\n",
        "        'world': 'general',  # 'world' maps to 'general' for broad news\n",
        "        'politics': 'politics',\n",
        "        'business': 'business',\n",
        "        'technology': 'technology',  # Corrected from 'tech'\n",
        "        'science': 'science',\n",
        "        'entertainment': 'entertainment',\n",
        "        'sports': 'sports',\n",
        "        'health': 'health'\n",
        "    }\n",
        "    mapped_category = category_map.get(category, 'general')\n",
        "\n",
        "    # Validate category\n",
        "    if mapped_category not in SUPPORTED_CATEGORIES:\n",
        "        logging.error(f\"Unsupported category '{mapped_category}'. Supported: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "        print(f\"Error: Category '{mapped_category}' not supported. Choose from: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "        return []\n",
        "\n",
        "    # Validate API key\n",
        "    if NEWSDATA_KEY == 'YOUR_NEWSDATA_KEY' or not NEWSDATA_KEY:\n",
        "        logging.error(\"Invalid or placeholder API key. Get a free key from https://newsdata.io/\")\n",
        "        print(\"Error: Set a valid NEWSDATA_KEY. Sign up at https://newsdata.io/ for free tier (500 calls/day).\")\n",
        "        return []\n",
        "\n",
        "    params = {\n",
        "        'category': mapped_category,\n",
        "        'apikey': NEWSDATA_KEY,\n",
        "        'language': 'en'\n",
        "    }\n",
        "    if country:\n",
        "        params['country'] = country\n",
        "\n",
        "    try:\n",
        "        response = requests.get('https://newsdata.io/api/1/news', params=params, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        data = response.json()\n",
        "\n",
        "        if data.get('status') != 'success':\n",
        "            error_msg = data.get('message', 'Unknown error')\n",
        "            if 'invalid_category' in str(error_msg).lower():\n",
        "                logging.error(f\"API rejected category '{mapped_category}'. Supported: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "                print(f\"Error: API rejected category '{mapped_category}'. Supported: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "            raise ValueError(f\"API error: {error_msg}\")\n",
        "\n",
        "        articles = []\n",
        "        for item in data.get('results', []):\n",
        "            article = {\n",
        "                'title': item.get('title', 'No Title'),\n",
        "                'description': item.get('description', 'No Description'),\n",
        "                'content': item.get('content', 'No Content'),\n",
        "                'link': item.get('link', 'No Link'),\n",
        "                'pub_date': item.get('pubDate', datetime.now().isoformat()),\n",
        "                'source': item.get('source_id', 'NewsData.io')\n",
        "            }\n",
        "            articles.append(article)\n",
        "\n",
        "        logging.info(f\"Fetched {len(articles)} articles from NewsData.io for {category} (mapped to '{mapped_category}')\")\n",
        "        print(f\"Success: Fetched {len(articles)} articles from NewsData.io for {category} (mapped to '{mapped_category}')\")\n",
        "        return articles\n",
        "\n",
        "    except requests.exceptions.HTTPError as e:\n",
        "        if e.response.status_code == 422:\n",
        "            error_detail = e.response.json() if e.response.content else {}\n",
        "            invalid_cat = error_detail.get('results', {}).get('invalid_category', mapped_category)\n",
        "            logging.error(f\"422 Error: Unsupported category '{invalid_cat}'. Supported: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "            print(f\"Error (422): Unsupported category '{invalid_cat}'. Supported: {', '.join(SUPPORTED_CATEGORIES)}\")\n",
        "        elif e.response.status_code == 429:\n",
        "            logging.error(\"429 Rate Limit Exceeded: Wait 24 hours or use a personal API key from https://newsdata.io/\")\n",
        "            print(\"Error (429): Rate limit exceeded. Wait 24 hours or get a personal key at https://newsdata.io/\")\n",
        "        else:\n",
        "            logging.error(f\"HTTP error: {str(e)}\")\n",
        "            print(f\"HTTP Error: {str(e)}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"NewsData.io fetch error for {category}: {str(e)}\")\n",
        "        print(f\"Error fetching NewsData.io for {category}: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "# Set your API key\n",
        "NEWSDATA_KEY = 'pub_f344b9f77d3645aabb54c0e52cb051fd'\n",
        "\n",
        "# Test with sample category\n",
        "sample_articles = fetch_newsdata('technology')\n",
        "print(f\"Sample article count: {len(sample_articles)}\")\n",
        "if sample_articles:\n",
        "    print(f\"First article title: {sample_articles[0]['title']}\")\n",
        "else:\n",
        "    print(\"No articles fetched. Try a different category like 'business' or check API key.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9REp-Sen3r-4"
      },
      "source": [
        "### Save articles to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eO05vMO53nrl",
        "outputId": "2e06fedb-8f00-4411-c310-b68bf8a0cf59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success: Saved 1 articles to /content/drive/MyDrive/news_data/test_category/test_20251026_205306.json\n"
          ]
        }
      ],
      "source": [
        "# Save articles to Google Drive as JSON\n",
        "def save_articles(articles, category, source):\n",
        "    try:\n",
        "        category_dir = os.path.join(BASE_DIR, category)\n",
        "        os.makedirs(category_dir, exist_ok=True)\n",
        "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        file_path = os.path.join(category_dir, f'{source}_{timestamp}.json')\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(articles, f, ensure_ascii=False, indent=4)\n",
        "        logging.info(f\"Saved {len(articles)} articles to {file_path}\")\n",
        "        print(f\"Success: Saved {len(articles)} articles to {file_path}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Save error for {file_path}: {str(e)}\")\n",
        "        print(f\"Error saving to {file_path}: {str(e)}\")\n",
        "\n",
        "# Test save function\n",
        "sample_articles = [{'title': 'Test Article', 'description': 'Test', 'content': 'Test content', 'link': 'http://example.com', 'pub_date': '2025-10-15T13:00:00Z', 'source': 'Test'}]\n",
        "save_articles(sample_articles, 'test_category', 'test')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkdNyie23ydW"
      },
      "source": [
        "### üîÅ Full Ingestion Loop\n",
        "\n",
        "This section orchestrates the **entire data ingestion process**, iterating over all configured categories to collect articles from both **RSS feeds** and **API sources**.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-iVuFgK3zkz",
        "outputId": "4e154c87-f31c-4482-9c59-3c86487e1f3d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing category: world\n",
            "Success: Fetched 36 articles from https://feeds.bbci.co.uk/news/world/rss.xml\n",
            "Success: Saved 36 articles to /content/drive/MyDrive/news_data/world/rss_20251026_205307.json\n",
            "Success: Fetched 0 articles from https://www.reuters.com/arc/outboundfeeds/world/?outputType=xml\n",
            "Success: Fetched 29 articles from http://rss.cnn.com/rss/cnn_world.rss\n",
            "Success: Saved 29 articles to /content/drive/MyDrive/news_data/world/rss_20251026_205307.json\n",
            "Success: Fetched 0 articles from NewsAPI for world\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:root:422 Error: Unsupported category 'general'. Supported: general, business, entertainment, health, science, sports, technology, politics\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error (422): Unsupported category 'general'. Supported: general, business, entertainment, health, science, sports, technology, politics\n",
            "Success: Saved 65 articles to /content/drive/MyDrive/news_data/world/combined_20251026_205308.json\n",
            "Total 65 articles saved for world\n",
            "Processing category: politics\n",
            "Success: Fetched 21 articles from https://www.theguardian.com/politics/rss\n",
            "Success: Saved 21 articles to /content/drive/MyDrive/news_data/politics/rss_20251026_205308.json\n",
            "Success: Fetched 0 articles from https://www.politico.com/rss/politicopulse.xml\n",
            "Success: Fetched 25 articles from https://www.aljazeera.com/xml/rss/all.xml\n",
            "Success: Saved 25 articles to /content/drive/MyDrive/news_data/politics/rss_20251026_205308.json\n",
            "Success: Fetched 18 articles from NewsAPI for politics\n",
            "Success: Saved 18 articles to /content/drive/MyDrive/news_data/politics/newsapi_20251026_205308.json\n",
            "Success: Fetched 10 articles from NewsData.io for politics (mapped to 'politics')\n",
            "Success: Saved 74 articles to /content/drive/MyDrive/news_data/politics/combined_20251026_205309.json\n",
            "Total 74 articles saved for politics\n",
            "Processing category: business\n",
            "Success: Fetched 0 articles from https://feeds.bloomberg.com/news/business.rss\n",
            "Success: Fetched 9 articles from https://www.ft.com/rss/home\n",
            "Success: Saved 9 articles to /content/drive/MyDrive/news_data/business/rss_20251026_205309.json\n",
            "Success: Fetched 30 articles from https://www.cnbc.com/id/100727362/device/rss/rss.html\n",
            "Success: Saved 30 articles to /content/drive/MyDrive/news_data/business/rss_20251026_205310.json\n",
            "Success: Fetched 19 articles from NewsAPI for business\n",
            "Success: Saved 19 articles to /content/drive/MyDrive/news_data/business/newsapi_20251026_205310.json\n",
            "Success: Fetched 10 articles from NewsData.io for business (mapped to 'business')\n",
            "Success: Saved 68 articles to /content/drive/MyDrive/news_data/business/combined_20251026_205311.json\n",
            "Total 68 articles saved for business\n",
            "Processing category: technology\n",
            "Success: Fetched 20 articles from https://techcrunch.com/feed/\n",
            "Success: Saved 20 articles to /content/drive/MyDrive/news_data/technology/rss_20251026_205311.json\n",
            "Success: Fetched 50 articles from https://www.wired.com/feed/rss\n",
            "Success: Saved 50 articles to /content/drive/MyDrive/news_data/technology/rss_20251026_205311.json\n",
            "Success: Fetched 10 articles from https://www.theverge.com/rss/index.xml\n",
            "Success: Saved 10 articles to /content/drive/MyDrive/news_data/technology/rss_20251026_205311.json\n",
            "Success: Fetched 20 articles from NewsAPI for technology\n",
            "Success: Saved 20 articles to /content/drive/MyDrive/news_data/technology/newsapi_20251026_205312.json\n",
            "Success: Fetched 10 articles from NewsData.io for technology (mapped to 'technology')\n",
            "Success: Saved 110 articles to /content/drive/MyDrive/news_data/technology/combined_20251026_205312.json\n",
            "Total 110 articles saved for technology\n",
            "Processing category: science\n",
            "Success: Fetched 75 articles from https://www.nature.com/nature.rss\n",
            "Success: Saved 75 articles to /content/drive/MyDrive/news_data/science/rss_20251026_205313.json\n",
            "Success: Fetched 10 articles from https://www.science.org/rss/news_current.xml\n",
            "Success: Saved 10 articles to /content/drive/MyDrive/news_data/science/rss_20251026_205313.json\n",
            "Success: Fetched 100 articles from https://www.newscientist.com/feed/home/?cmpid=RSS%7CNSNS%7Cnews\n",
            "Success: Saved 100 articles to /content/drive/MyDrive/news_data/science/rss_20251026_205313.json\n",
            "Success: Fetched 20 articles from NewsAPI for science\n",
            "Success: Saved 20 articles to /content/drive/MyDrive/news_data/science/newsapi_20251026_205313.json\n",
            "Success: Fetched 10 articles from NewsData.io for science (mapped to 'science')\n",
            "Success: Saved 215 articles to /content/drive/MyDrive/news_data/science/combined_20251026_205314.json\n",
            "Total 215 articles saved for science\n",
            "Processing category: entertainment\n",
            "Success: Fetched 10 articles from https://variety.com/feed/\n",
            "Success: Saved 10 articles to /content/drive/MyDrive/news_data/entertainment/rss_20251026_205314.json\n",
            "Success: Fetched 10 articles from https://www.hollywoodreporter.com/feed/\n",
            "Success: Saved 10 articles to /content/drive/MyDrive/news_data/entertainment/rss_20251026_205314.json\n",
            "Success: Fetched 22 articles from https://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml\n",
            "Success: Saved 22 articles to /content/drive/MyDrive/news_data/entertainment/rss_20251026_205314.json\n",
            "Success: Fetched 19 articles from NewsAPI for entertainment\n",
            "Success: Saved 19 articles to /content/drive/MyDrive/news_data/entertainment/newsapi_20251026_205315.json\n",
            "Success: Fetched 10 articles from NewsData.io for entertainment (mapped to 'entertainment')\n",
            "Success: Saved 71 articles to /content/drive/MyDrive/news_data/entertainment/combined_20251026_205315.json\n",
            "Total 71 articles saved for entertainment\n",
            "Processing category: sports\n",
            "Success: Fetched 27 articles from https://www.espn.com/espn/rss/news\n",
            "Success: Saved 27 articles to /content/drive/MyDrive/news_data/sports/rss_20251026_205315.json\n",
            "Success: Fetched 80 articles from https://feeds.bbci.co.uk/sport/rss.xml\n",
            "Success: Saved 80 articles to /content/drive/MyDrive/news_data/sports/rss_20251026_205316.json\n",
            "Success: Fetched 20 articles from https://www.skysports.com/rss/0,20514,11095,00.xml\n",
            "Success: Saved 20 articles to /content/drive/MyDrive/news_data/sports/rss_20251026_205316.json\n",
            "Success: Fetched 20 articles from NewsAPI for sports\n",
            "Success: Saved 20 articles to /content/drive/MyDrive/news_data/sports/newsapi_20251026_205316.json\n",
            "Success: Fetched 10 articles from NewsData.io for sports (mapped to 'sports')\n",
            "Success: Saved 157 articles to /content/drive/MyDrive/news_data/sports/combined_20251026_205316.json\n",
            "Total 157 articles saved for sports\n",
            "Processing category: health\n",
            "Success: Fetched 0 articles from https://www.who.int/feeds/entity/mediacentre/news/en/rss.xml\n",
            "Success: Fetched 0 articles from https://www.reuters.com/arc/outboundfeeds/health/?outputType=xml\n",
            "Success: Fetched 0 articles from https://newsnetwork.mayoclinic.org/feed/\n",
            "Success: Fetched 19 articles from NewsAPI for health\n",
            "Success: Saved 19 articles to /content/drive/MyDrive/news_data/health/newsapi_20251026_205317.json\n",
            "Success: Fetched 10 articles from NewsData.io for health (mapped to 'health')\n",
            "Success: Saved 29 articles to /content/drive/MyDrive/news_data/health/combined_20251026_205318.json\n",
            "Total 29 articles saved for health\n"
          ]
        }
      ],
      "source": [
        "# Fetch and save articles for all categories\n",
        "for category in CATEGORIES:\n",
        "    print(f\"Processing category: {category}\")\n",
        "    all_articles = []\n",
        "\n",
        "    # RSS feeds\n",
        "    for feed_url in RSS_FEEDS.get(category, []):\n",
        "        articles = fetch_rss(feed_url)\n",
        "        if articles:\n",
        "            all_articles.extend(articles)\n",
        "            save_articles(articles, category, 'rss')\n",
        "\n",
        "    # NewsAPI\n",
        "    articles = fetch_newsapi(category)\n",
        "    if articles:\n",
        "        all_articles.extend(articles)\n",
        "        save_articles(articles, category, 'newsapi')\n",
        "\n",
        "    # NewsData.io\n",
        "    articles = fetch_newsdata(category)\n",
        "    if articles:\n",
        "        all_articles.extend(articles)\n",
        "\n",
        "    # Save combined articles\n",
        "    if all_articles:\n",
        "        save_articles(all_articles, category, 'combined')\n",
        "        print(f\"Total {len(all_articles)} articles saved for {category}\")\n",
        "    else:\n",
        "        print(f\"No articles fetched for {category}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvZapaHh37cW"
      },
      "source": [
        "### Verify saved files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rL8Nqwze3-aU",
        "outputId": "7139ecdc-dad4-462e-9a56-f26df509b612"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample article from /content/drive/MyDrive/news_data/technology/combined_20251026_205312.json:\n",
            "{\n",
            "    \"title\": \"Ads might be coming to Apple Maps next year\",\n",
            "    \"description\": \"This could be part of a larger strategy to introduce more advertising in iOS.\",\n",
            "    \"content\": \"No Content\",\n",
            "    \"link\": \"https://techcrunch.com/2025/10/26/ads-might-be-coming-to-apple-maps-next-year/\",\n",
            "    \"pub_date\": \"Sun, 26 Oct 2025 19:38:39 +0000\",\n",
            "    \"source\": \"TechCrunch\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Verify saved files by displaying a sample article\n",
        "import glob\n",
        "\n",
        "sample_file = glob.glob(f'{BASE_DIR}/technology/combined_*.json')\n",
        "if sample_file:\n",
        "    try:\n",
        "        with open(sample_file[0], 'r', encoding='utf-8') as f:\n",
        "            sample_data = json.load(f)\n",
        "        print(f\"Sample article from {sample_file[0]}:\")\n",
        "        print(json.dumps(sample_data[0], indent=4))\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {sample_file[0]}: {str(e)}\")\n",
        "else:\n",
        "    print(\"No sample files found. Run ingestion first or check API keys.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JFLrycXSzNx"
      },
      "source": [
        "## üì∞ Data Preprocessing\n",
        "\n",
        " This section is responsible for cleaning, and structuring the raw data into a consistent format.\n",
        "\n",
        "Preprocessing includes:\n",
        "- Removing unnecessary HTML tags and symbols  \n",
        "- Normalizing whitespace and punctuation  \n",
        "- Deduplicating similar content using TF-IDF similarity thresholds  \n",
        "- Chunking long articles into smaller pieces to improve retrieval precision  \n",
        "\n",
        "The final output of this step is a list of clean, concise text chunks that are ready to be embedded.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "HV4dkkyIS6s4"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install beautifulsoup4 nltk rouge-score datasets --quiet\n",
        "import json\n",
        "import os\n",
        "import glob\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import logging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXdxECgpTG0g",
        "outputId": "d98c8557-863a-4d31-da22-0974ce342faf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK 'punkt' data (and punkt_tab fallback) ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK downloads complete.\n",
            "‚úÖ Tokenization OK. Example: ['This is a test.', 'It has two sentences.']\n"
          ]
        }
      ],
      "source": [
        "# NLTK fix: ensure punkt tokenizer is available (required for NLTK >= 3.8)\n",
        "print(\"Downloading NLTK 'punkt' data (and punkt_tab fallback) ...\")\n",
        "nltk.download('punkt_tab', quiet=False)\n",
        "nltk.download('punkt', quiet=False)\n",
        "print(\"NLTK downloads complete.\")\n",
        "\n",
        "# Quick sent_tokenize smoke test\n",
        "try:\n",
        "    test_sentences = sent_tokenize(\"This is a test. It has two sentences.\")\n",
        "    print(f\"‚úÖ Tokenization OK. Example: {test_sentences}\")\n",
        "except LookupError as e:\n",
        "    print(f\"‚ùå Tokenization error: {e}\")\n",
        "    print(\"Manual fix: run nltk.download('punkt_tab') in a separate cell if needed.\")\n",
        "    raise e\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "BASE_DIR = '/content/drive/MyDrive/news_data'\n",
        "\n",
        "def load_articles(category):\n",
        "    \"\"\"Load all combined JSON articles for a category.\"\"\"\n",
        "    files = glob.glob(f'{BASE_DIR}/{category}/combined_*.json')\n",
        "    all_articles = []\n",
        "    for file in files:\n",
        "        with open(file, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            all_articles.extend(data)\n",
        "    logging.info(f\"Loaded {len(all_articles)} articles for {category}\")\n",
        "    return all_articles\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8A-7ZiPSfMg"
      },
      "source": [
        "1. **Text Cleaning**\n",
        "   ```\n",
        "   Raw HTML ‚Üí BeautifulSoup4 ‚Üí Clean Text ‚Üí Normalized Whitespace\n",
        "   ```\n",
        "   - HTML tag removal\n",
        "   - Special character handling\n",
        "   - Unicode normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sKvGN7HDTMiQ"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    \"\"\"Clean text: remove HTML and normalize whitespace.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRqGMT1zSfMh"
      },
      "source": [
        "2. **Deduplication**\n",
        "   ```\n",
        "   Articles ‚Üí TF-IDF Vectors ‚Üí Cosine Similarity ‚Üí Unique Articles\n",
        "   ```\n",
        "   - TF-IDF vectorization (scikit-learn)\n",
        "   - Pairwise similarity computation\n",
        "   - Threshold-based filtering (0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "d1DFKeH5TXiz"
      },
      "outputs": [],
      "source": [
        "def deduplicate_articles(articles, threshold=0.8):\n",
        "    \"\"\"Deduplicate articles based on TF-IDF similarity. Handles None values.\"\"\"\n",
        "    if len(articles) < 2:\n",
        "        return articles\n",
        "    # Safety: use .get() and str() to avoid None values\n",
        "    texts = [clean_text(str(a.get('title', '')) + ' ' + str(a.get('description', '')) + ' ' + str(a.get('content', ''))) for a in articles]\n",
        "    texts = [t for t in texts if t.strip()]  # Ignore empty texts\n",
        "    if not texts:\n",
        "        return articles\n",
        "    vectorizer = TfidfVectorizer().fit_transform(texts)\n",
        "    sim_matrix = cosine_similarity(vectorizer)\n",
        "    unique_indices = [0]\n",
        "    for i in range(1, len(articles)):\n",
        "        if all(sim_matrix[i][j] < threshold for j in unique_indices):\n",
        "            unique_indices.append(i)\n",
        "    unique_articles = [articles[i] for i in unique_indices]\n",
        "    logging.info(f\"Deduplicated: {len(articles)} -> {len(unique_articles)}\")\n",
        "    return unique_articles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENBWirCwSfMh"
      },
      "source": [
        "3. **Content Chunking**\n",
        "   ```\n",
        "   Clean Text ‚Üí NLTK Sentences ‚Üí Fixed-Size Chunks ‚Üí Metadata\n",
        "   ```\n",
        "   - Sentence boundary detection\n",
        "   - Length-aware chunking (500 chars)\n",
        "   - Metadata preservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Pc88nO0UTbpm"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text, max_length=500):\n",
        "    \"\"\"Split text into chunks of at most max_length characters.\"\"\"\n",
        "    sentences = sent_tokenize(text)\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "    for sent in sentences:\n",
        "        if len(current_chunk + sent) < max_length:\n",
        "            current_chunk += sent + \" \"\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = sent + \" \"\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "tlAEzH5JWE53"
      },
      "outputs": [],
      "source": [
        "def preprocess_category(category):\n",
        "    \"\"\"Full preprocessing pipeline for a category. Handles None values.\"\"\"\n",
        "    articles = load_articles(category)\n",
        "    articles = deduplicate_articles(articles)\n",
        "\n",
        "    processed = []\n",
        "    for art in articles:\n",
        "        full_text = clean_text(str(art.get('title', '')) + ' ' + str(art.get('description', '')) + ' ' + str(art.get('content', '')))\n",
        "        chunks = chunk_text(full_text)\n",
        "        for i, chunk in enumerate(chunks):\n",
        "            if chunk.strip():  # Ignore empty chunks\n",
        "                processed.append({\n",
        "                    'id': f\"{art.get('link', 'unknown')}_{i}\",\n",
        "                    'title': art.get('title', 'No Title'),\n",
        "                    'source': art.get('source', 'Unknown'),\n",
        "                    'link': art.get('link', 'No Link'),\n",
        "                    'content': chunk,\n",
        "                    'category': category\n",
        "                })\n",
        "\n",
        "    output_file = f'{BASE_DIR}/{category}_processed.json'\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(processed, f, ensure_ascii=False, indent=2)\n",
        "    logging.info(f\"Preprocessing complete: {len(processed)} chunks saved to {output_file}\")\n",
        "    return processed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw23bIu4V_-X",
        "outputId": "364b8570-5b81-4e5f-9052-79b3158e9049"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample chunk: Ads might be coming to Apple Maps next year This could be part of a larger strategy to introduce more advertising in iOS. No Content...\n"
          ]
        }
      ],
      "source": [
        "# Test on a category (should work now)\n",
        "processed_tech = preprocess_category('technology')\n",
        "if processed_tech:\n",
        "    print(f\"Sample chunk: {processed_tech[0]['content'][:200]}...\")\n",
        "else:\n",
        "    print(\"No chunks generated ‚Äì check the source data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mtOThZIUXHt1"
      },
      "source": [
        "## üîπ Generating Embeddings\n",
        "\n",
        "In this step, we transform text chunks into **numerical vector representations** using the pre-trained **Sentence-Transformer model `all-MiniLM-L6-v2`**.  \n",
        "\n",
        "These embeddings form the foundation of our **vector database**, which powers the retrieval stage of the RAG pipeline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LDRGokLLXI40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e9991c5-70cd-491a-b40c-f9284dae1b5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install for embeddings and vector store\n",
        "!pip install sentence-transformers faiss-cpu --quiet\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import numpy as np\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGN-yglvSfMi"
      },
      "source": [
        "### Components\n",
        "\n",
        "1. **Embedding Model**\n",
        "   ```\n",
        "   Model: all-MiniLM-L6-v2\n",
        "   Output: 384-dimensional vectors\n",
        "   Framework: sentence-transformers\n",
        "   ```\n",
        "   - Optimized for semantic similarity\n",
        "   - Multi-lingual support\n",
        "   - Efficient inference\n",
        "\n",
        "2. **FAISS Integration**\n",
        "   ```\n",
        "   Vectors ‚Üí FAISS FlatL2 ‚Üí Indexed Storage\n",
        "   ```\n",
        "   - Exact nearest neighbor search\n",
        "   - L2 distance metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXctWX5MXTCP"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "def create_embeddings(chunks):\n",
        "    \"\"\"Generate embeddings for the chunks.\"\"\"\n",
        "    embeddings = model.encode([c['content'] for c in chunks])\n",
        "    return np.array(embeddings).astype('float32')\n",
        "\n",
        "def build_faiss_index(chunks, embeddings):\n",
        "    \"\"\"Build and save a FAISS index.\"\"\"\n",
        "    dimension = embeddings.shape[1]\n",
        "    index = faiss.IndexFlatL2(dimension)\n",
        "    index.add(embeddings)\n",
        "\n",
        "    faiss.write_index(index, f'{BASE_DIR}/{chunks[0][\"category\"]}_faiss.index')\n",
        "    with open(f'{BASE_DIR}/{chunks[0][\"category\"]}_chunks.pkl', 'wb') as f:\n",
        "        pickle.dump(chunks, f)\n",
        "    logging.info(f\"FAISS index built: {len(chunks)} vectors\")\n",
        "    return index\n",
        "\n",
        "processed_tech = json.load(open(f'{BASE_DIR}/technology_processed.json', 'r'))\n",
        "embeddings_tech = create_embeddings(processed_tech)\n",
        "index_tech = build_faiss_index(processed_tech, embeddings_tech)\n",
        "print(\"FAISS index ready for retrieval!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y60yLzHgSfMv"
      },
      "source": [
        "## üîç Building the RAG Pipeline\n",
        "\n",
        "This section combines all components : retriever, embeddings, and LLM into a **Retrieval-Augmented Generation (RAG)** chain.  \n",
        "\n",
        "When a user asks a question:\n",
        "1. The retriever fetches the most relevant text chunks from the vector store.  \n",
        "2. These chunks are inserted into the prompt as contextual evidence.  \n",
        "3. The language model then generates an answer grounded in the retrieved context.\n",
        "\n",
        "This approach minimizes hallucinations and ensures the chatbot provides fact-based responses backed by real news data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkMjVE2cSfMv"
      },
      "source": [
        "### ‚öôÔ∏è Underlying RAG System Components\n",
        "\n",
        "#### 1. **Retrieval Component**\n",
        "- Performs **Top-k semantic search (k=5)** using FAISS  \n",
        "- Applies **score normalization** to improve ranking precision  \n",
        "- Attaches **metadata** (title, source, URL) to retrieved documents  \n",
        "\n",
        "#### 2. **Generation Pipeline**\n",
        "- Uses **Qwen2-1.5B-Instruct**, a multilingual instruction-tuned model  \n",
        "- Hardware Optimization:\n",
        "     - Float16 quantization (GPU)\n",
        "     - Automatic device mapping\n",
        "\n",
        "#### 3. **Prompt Engineering**\n",
        "- Injects a structured **system context** for every query  \n",
        "- Enforces **source citation** in generated responses  \n",
        "- Supports **French and English** through multilingual optimization  \n",
        "- Temperature control **0.7**\n",
        "\n",
        "#### 4. **Integration Architecture**\n",
        "- Built on a **LangChain RetrievalQA** chain  \n",
        "- Uses **custom prompt templates** for consistent formatting  \n",
        "- Tracks **source documents** for transparency  \n",
        "- Includes **error handling** for robust operation  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "06k47BLHYM46"
      },
      "outputs": [],
      "source": [
        "def retrieve_chunks(query, index, chunks, model, k=5):\n",
        "    \"\"\"Retrieve top-k chunks for a query.\"\"\"\n",
        "    query_emb = model.encode([query]).astype('float32')\n",
        "    distances, indices = index.search(query_emb, k)\n",
        "    results = []\n",
        "    for dist, idx in zip(distances[0], indices[0]):\n",
        "        if idx < len(chunks):\n",
        "            results.append({\n",
        "                'chunk': chunks[idx]['content'],\n",
        "                'source': chunks[idx]['source'],\n",
        "                'link': chunks[idx]['link'],\n",
        "                'score': 1 - dist\n",
        "            })\n",
        "    return results\n",
        "\n",
        "# Test retrieval\n",
        "query = \"Latest AI advancements in 2025\"\n",
        "retrieved = retrieve_chunks(query, index_tech, processed_tech, model)\n",
        "for r in retrieved:\n",
        "    print(f\"Score: {r['score']:.2f} | Source: {r['source']} | Chunk: {r['chunk'][:100]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eo3Kei3jlSy5"
      },
      "outputs": [],
      "source": [
        "# Install LangChain and transformers\n",
        "!pip install langchain langchain-community langchain-huggingface transformers accelerate bitsandbytes sentence-transformers faiss-cpu --quiet\n",
        "import torch\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_huggingface import HuggingFacePipeline, HuggingFaceEmbeddings\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.schema import Document\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# Check GPU availability\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "# Load Qwen2-1.5B-Instruct\n",
        "model_name = \"Qwen/Qwen2-1.5B-Instruct\"\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.float16 if device == 0 else torch.float32,\n",
        "        device_map=\"auto\" if device == 0 else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.7,\n",
        "        do_sample=True,\n",
        "        device=device\n",
        "    )\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "    print(\"‚úÖ Qwen loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Qwen error ({e}). Falling back to GPT-2...\")\n",
        "    from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=150, device=device)\n",
        "    llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "# Key step: build LangChain vectorstore\n",
        "# Convert chunks into LangChain Document objects\n",
        "docs = [Document(\n",
        "    page_content=chunk['content'],\n",
        "    metadata={\n",
        "        'title': chunk['title'],\n",
        "        'source': chunk['source'],\n",
        "        'link': chunk['link'],\n",
        "        'category': chunk['category']\n",
        "    }\n",
        ") for chunk in processed_tech]\n",
        "\n",
        "# LangChain embeddings wrapper (uses sentence-transformers model)\n",
        "embeddings = HuggingFaceEmbeddings(model_name='all-MiniLM-L6-v2')\n",
        "\n",
        "# Build FAISS vectorstore\n",
        "vectorstore = FAISS.from_documents(docs, embeddings)\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "\n",
        "print(\"‚úÖ Vectorstore and retriever are ready!\")\n",
        "\n",
        "# Prompt template for Qwen\n",
        "prompt_template = \"\"\"<|im_start|>system\n",
        "Tu es un assistant news expert. R√©ponds de mani√®re concise et factuelle en fran√ßais, bas√© uniquement sur le contexte fourni. Cite les sources via metadata (source, link).\n",
        "<|im_end|>\n",
        "<|im_start|>user\n",
        "Contexte: {context}\n",
        "\n",
        "Question: {question}\n",
        "<|im_end|>\n",
        "<|im_start|>assistant\n",
        "\"\"\"\n",
        "\n",
        "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "\n",
        "# RAG chain (with valid retriever)\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    return_source_documents=True,\n",
        "    chain_type_kwargs={\"prompt\": PROMPT}\n",
        ")\n",
        "\n",
        "# Simplified RAG function (uses LangChain chain)\n",
        "def rag_query(query):\n",
        "    \"\"\"Run RAG using the Qwen model and the integrated retriever.\"\"\"\n",
        "    result = qa_chain.invoke({\"query\": query})\n",
        "    sources = []\n",
        "    for doc in result.get('source_documents', []):\n",
        "        sources.append({\n",
        "            'chunk': doc.page_content,\n",
        "            'source': doc.metadata['source'],\n",
        "            'link': doc.metadata['link'],\n",
        "            'score': 0.0\n",
        "        })\n",
        "    return {\n",
        "        \"answer\": result['result'].strip(),\n",
        "        \"sources\": sources\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjFhC6d5SfMw"
      },
      "source": [
        "## üß™ Demonstration\n",
        "\n",
        "This final section showcases the chatbot‚Äôs functionality.  \n",
        "By entering a natural language question, the user receives an AI-generated answer based on the most relevant news content in the dataset.  \n",
        "\n",
        "The output includes:\n",
        "- The **question** posed  \n",
        "- The **AI-generated answer**  \n",
        "- The **source articles** that supported the response  \n",
        "\n",
        "This demo validates the effectiveness of our RAG pipeline and completes the full end-to-end workflow of our intelligent news chatbot.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_VatX9wSfMw",
        "outputId": "789f85d8-e1d5-4f69-cd03-5ff0011fdc01"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**Question:** Connaissez-vous l'√©cole d'ing√©nieurs ESPRIT en Tunisie ? \n",
            "**R√©ponse (Qwen):** <|im_start|>system\n",
            "Tu es un assistant news expert. R√©ponds de mani√®re concise et factuelle en fran√ßais, bas√© uniquement sur le contexte fourni. Cite les sources via metadata (source, link).\n",
            "<|im_end|>\n",
            "<|im_start|>user\n",
            "Contexte: As Reliance, Meta announce Rs 855 crore AI partnership; RIL clarifies that the transaction does not ... Reliance Industries Limited and Meta Platforms are joining forces. They have formed a new company called Reliance Enterprise Intelligence Limited. This new entity will focus on developing and distributing artificial intelligence services for businesses. The partnership aims to strengthen enterprise technology capabilities and explore new AI solutions.\n",
            "\n",
            "Meet the Palestinian Teens Trying to Win Robotics Gold Next week, five teens from Palestine will head to Panama to compete in one of the world‚Äôs largest youth robotics competitions. The goal? To win‚Äîand then teach STEM to their peers displaced by the Israel-Hamas war. No Content\n",
            "\n",
            "Wordle has achievements now - The Verge Spelling Bee and Connections have them, too. Spelling Bee and Connections have them, too. Spelling Bee and Connections have them, too. by Jay PetersClose Jay Peters Posts from this author will be added to your ‚Ä¶ [+1630 chars]\n",
            "\n",
            "Question: Connaissez-vous l'√©cole d'ing√©nieurs ESPRIT en Tunisie ? \n",
            "<|im_end|>\n",
            "<|im_start|>assistant\n",
            "Oui, je connais l'√©cole d'ing√©nieurs ESPRIT en Tunisie. Elle est reconnue internationalement pour son enseignement de qualit√© et sa contribution √† la formation des professionnels de l'industrie. Vous pouvez consulter leur site web officiel pour plus d'informations : https://www.espirtunisia.edu.tn/\n",
            "\n",
            "**Sources:**\n",
            "- toi: https://timesofindia.indiatimes.com/technology/tech-news/as-reliance-meta-announce-rs-855-crore-ai-partnership-ril-clarifies-that-the-transaction-does-not-/articleshow/124824940.cms\n",
            "- WIRED: https://www.wired.com/story/meet-the-palestinian-teens-trying-to-win-robotics-gold/\n",
            "- The Verge: https://www.theverge.com/news/806578/nyt-games-badges-achievements-wordle-spelling-bee-connections\n"
          ]
        }
      ],
      "source": [
        "# Test RAG\n",
        "query = \"Connaissez-vous l'√©cole d'ing√©nieurs ESPRIT en Tunisie ? \"\n",
        "result = rag_query(query)\n",
        "print(\"**Question:**\", query)\n",
        "print(\"**R√©ponse (Qwen):**\", result['answer'])\n",
        "print(\"\\n**Sources:**\")\n",
        "for src in result['sources']:\n",
        "    print(f\"- {src['source']}: {src['link']}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}